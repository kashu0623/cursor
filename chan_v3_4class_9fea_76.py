import random
import numpy as np
import torch
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
import glob
import logging
from collections import Counter
from scipy.signal import welch, find_peaks  # PPG ì‹ í˜¸ ë¶„ì„ì„ ìœ„í•œ scipy.signal ì¶”ê°€
from scipy.integrate import trapezoid  # ì ë¶„ì„ ìœ„í•œ trapezoid í•¨ìˆ˜ ì¶”ê°€

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ì „ì—­ ì„¤ì • ìƒìˆ˜
SAMPLING_RATE = 64.0        # Hz
WINDOW_DURATION = 30        # seconds
WINDOW_SIZE = int(SAMPLING_RATE * WINDOW_DURATION)  # 1920 samples
# [ìˆ˜ì •] 4í´ë˜ìŠ¤ ë¶„ë¥˜ (W, Light, N3, R)
NUM_CLASSES = 4             # ìˆ˜ë©´ ë‹¨ê³„ í´ë˜ìŠ¤ ìˆ˜ (W, Light, N3, R)
MAJORITY_RATIO = 0.6        # majority voting ë¹„ìœ¨

# í˜„ì¬ ì‚¬ìš©í•  ì‹ í˜¸ ì±„ë„ ì„¤ì • (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
CURRENT_SIGNAL_CHANNELS = ['bvp', 'acc_x', 'acc_y', 'acc_z']  # í˜„ì¬ 4ì±„ë„
CURRENT_LABEL_COLUMN = 'sleep_stage'  # í˜„ì¬ ë¼ë²¨ ì»¬ëŸ¼ëª…

# í–¥í›„ í™•ì¥ìš© ì‹ í˜¸ ì±„ë„ (ì£¼ì„ ì²˜ë¦¬)
# FUTURE_SIGNAL_CHANNELS = ['ir', 'red', 'acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']  # 8ì±„ë„
# FUTURE_LABEL_COLUMN = 'label'  # í–¥í›„ ë¼ë²¨ ì»¬ëŸ¼ëª…

# í•™ìŠµ ì„¤ì • ìƒìˆ˜
DREAMT_DATA_DIR = "path/to/DREAMT"
ACTUAL_DATA_DIR = "path/to/actual_data"
PRETRAINED_PATH = "dreamt_pretrained.pth"
FINETUNED_PATH = "finetuned_model.pth"
EPOCHS_PRETRAIN = 30
EPOCHS_FINETUNE = 20
BATCH_SIZE = 32
LR_PRETRAIN = 1e-3
LR_FINETUNE = 1e-5

LABEL_MAP = {
    'W': 0, 'P': 0,    # Wake: 0
    'N1': 1, 'N2': 1,  # Light Sleep: 1
    'N3': 2,           # Deep Sleep: 2
    'R': 3, 'REM': 3   # REM Sleep: 3
}

INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}

# [ìˆ˜ì •] 4í´ë˜ìŠ¤ìš© ì´ë¦„ ë¦¬ìŠ¤íŠ¸
CLASS_NAMES_4 = ['Wake', 'Light', 'N3', 'REM']


def find_ppg_peaks(bvp_signal, sampling_rate=64.0, distance_min=0.5, prominence=0.1):
    """
    PPG ì‹ í˜¸(BVP)ì—ì„œ ì‹¬ë°•ìˆ˜ í”¼í¬ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Args:
        bvp_signal (np.array): BVP ì‹ í˜¸ ë°ì´í„° (1ì°¨ì› ë°°ì—´)
        sampling_rate (float): ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz, ê¸°ë³¸ê°’: 64.0)
        distance_min (float): í”¼í¬ ê°„ ìµœì†Œ ê±°ë¦¬ (ì´ˆ, ê¸°ë³¸ê°’: 0.5)
        prominence (float): í”¼í¬ì˜ ìµœì†Œ prominence (ê¸°ë³¸ê°’: 0.1)
        
    Returns:
        tuple: (peak_indices, peak_properties)
            - peak_indices: í”¼í¬ ìœ„ì¹˜ ì¸ë±ìŠ¤ ë°°ì—´
            - peak_properties: í”¼í¬ ì†ì„± ë”•ì…”ë„ˆë¦¬
    """
    # ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ ê³ ë ¤í•œ ìµœì†Œ ê±°ë¦¬ ê³„ì‚° (ìƒ˜í”Œ ë‹¨ìœ„)
    distance_samples = int(distance_min * sampling_rate)
    
    # scipy.signal.find_peaksë¥¼ ì‚¬ìš©í•˜ì—¬ í”¼í¬ ê²€ì¶œ
    peak_indices, peak_properties = find_peaks(
        bvp_signal,
        distance=distance_samples,
        prominence=prominence,
        height=None,  # ë†’ì´ ì œí•œ ì—†ìŒ
        width=None    # ë„ˆë¹„ ì œí•œ ì—†ìŒ
    )
    
    return peak_indices, peak_properties


def calculate_frequency_features(ibi_seconds):
    """
    IBI ë°ì´í„°ë¡œë¶€í„° ì£¼íŒŒìˆ˜ í”¼ì²˜(VLF, LF, HF power, LF/HF ratio)ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        ibi_seconds (np.array): IBI ë°ì´í„° (ì´ˆ ë‹¨ìœ„)
        
    Returns:
        dict: ì£¼íŒŒìˆ˜ ì˜ì—­ HRV í”¼ì²˜ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            - vlf: Very Low Frequency power (0.003-0.04 Hz)
            - lf: Low Frequency power (0.04-0.15 Hz)
            - hf: High Frequency power (0.15-0.4 Hz)
            - lf_hf_ratio: LF/HF ratio
    """
    if len(ibi_seconds) < 10:
        return {'vlf': 0.0, 'lf': 0.0, 'hf': 0.0, 'lf_hf_ratio': 0.0}
    
    fs = 4.0
    steps = 1 / fs
    x_interp = np.arange(ibi_seconds.min(), ibi_seconds.max(), steps)
    y_interp = np.interp(x_interp, np.cumsum(ibi_seconds), ibi_seconds)
    
    f, Pxx = welch(y_interp, fs=fs, nperseg=len(y_interp))
    
    vlf_band, lf_band, hf_band = (0.003, 0.04), (0.04, 0.15), (0.15, 0.4)
    
    vlf_power = trapezoid(Pxx[(f >= vlf_band[0]) & (f < vlf_band[1])], f[(f >= vlf_band[0]) & (f < vlf_band[1])])
    lf_power = trapezoid(Pxx[(f >= lf_band[0]) & (f < lf_band[1])], f[(f >= lf_band[0]) & (f < lf_band[1])])
    hf_power = trapezoid(Pxx[(f >= hf_band[0]) & (f < hf_band[1])], f[(f >= hf_band[0]) & (f < hf_band[1])])
    
    lf_hf_ratio = lf_power / hf_power if hf_power > 0 else 0.0
    
    return {'vlf': vlf_power, 'lf': lf_power, 'hf': hf_power, 'lf_hf_ratio': lf_hf_ratio}


def calculate_all_features(bvp_signal, sampling_rate=64.0, distance_min=0.5, prominence=0.1):
    """
    PPG ì‹ í˜¸ì—ì„œ ì‹œê°„ ë° ì£¼íŒŒìˆ˜ ì˜ì—­ HRV í”¼ì²˜ë¥¼ ëª¨ë‘ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        bvp_signal (np.array): BVP ì‹ í˜¸ ë°ì´í„° (1ì°¨ì› ë°°ì—´)
        sampling_rate (float): ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz, ê¸°ë³¸ê°’: 64.0)
        distance_min (float): í”¼í¬ ê°„ ìµœì†Œ ê±°ë¦¬ (ì´ˆ, ê¸°ë³¸ê°’: 0.5)
        prominence (float): í”¼í¬ì˜ ìµœì†Œ prominence (ê¸°ë³¸ê°’: 0.1)
        
    Returns:
        dict: ì‹œê°„ ë° ì£¼íŒŒìˆ˜ ì˜ì—­ HRV í”¼ì²˜ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            - hr: ë¶„ë‹¹ ì‹¬ë°•ìˆ˜ (float)
            - rmssd: RMSSD HRV ì§€í‘œ (float)
            - peak_count: ê²€ì¶œëœ í”¼í¬ ìˆ˜ (int)
            - ibi_mean: í‰ê·  IBI (ì´ˆ, float)
            - ibi_std: IBI í‘œì¤€í¸ì°¨ (ì´ˆ, float)
            - vlf: Very Low Frequency power (0.003-0.04 Hz)
            - lf: Low Frequency power (0.04-0.15 Hz)
            - hf: High Frequency power (0.15-0.4 Hz)
            - lf_hf_ratio: LF/HF ratio
    """
    peak_indices, _ = find_ppg_peaks(bvp_signal, sampling_rate, distance_min, prominence)
    
    # ê¸°ë³¸ í”¼ì²˜ ì´ˆê¸°í™”
    time_features = {
        'hr': 0.0, 
        'rmssd': 0.0, 
        'peak_count': len(peak_indices), 
        'ibi_mean': 0.0, 
        'ibi_std': 0.0
    }
    freq_features = {
        'vlf': 0.0, 
        'lf': 0.0, 
        'hf': 0.0, 
        'lf_hf_ratio': 0.0
    }
    
    if len(peak_indices) >= 2:
        ibi_seconds = np.diff(peak_indices) / sampling_rate
        
        if len(ibi_seconds) > 0:
            ibi_mean = np.mean(ibi_seconds)
            time_features['hr'] = 60.0 / ibi_mean if ibi_mean > 0 else 0.0
            time_features['ibi_mean'] = ibi_mean
            time_features['ibi_std'] = np.std(ibi_seconds)
            
            if len(ibi_seconds) >= 2:
                time_features['rmssd'] = np.sqrt(np.mean(np.diff(ibi_seconds) ** 2))
            
            freq_features = calculate_frequency_features(ibi_seconds)
    
    return {**time_features, **freq_features}


def map_sleep_stage(raw_label):
    return LABEL_MAP.get(str(raw_label).strip().upper(), None)


def extract_epochs_with_features_from_df(df, majority_ratio=MAJORITY_RATIO, signal_channels=None, label_column=None):
    """
    DataFrameì—ì„œ epochë³„ë¡œ raw signal, HR/HRV í”¼ì²˜, ëŒ€í‘œ ë¼ë²¨ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (majority voting)
    
    Args:
        df (pd.DataFrame): DataFrame with signal and label columns
        majority_ratio (float): majority voting ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.6)
        signal_channels (list): ì‚¬ìš©í•  ì‹ í˜¸ ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: CURRENT_SIGNAL_CHANNELS)
        label_column (str): ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: CURRENT_LABEL_COLUMN)
        
    Returns:
        tuple: (raw_signals, features, labels) 
            - raw_signals: np.array of shape (N, num_channels, 1920) - ì›ì‹œ ì‹ í˜¸ ë°ì´í„°
            - features: np.array of shape (N, 9) - HR/HRV í”¼ì²˜ (HR, RMSSD, PeakCount, IBI_mean, IBI_std, VLF, LF, HF, LF/HF_ratio)
            - labels: np.array of shape (N,) - ìˆ˜ë©´ ë‹¨ê³„ ë¼ë²¨
    """
    if signal_channels is None:
        signal_channels = CURRENT_SIGNAL_CHANNELS
    if label_column is None:
        label_column = CURRENT_LABEL_COLUMN
        
    raw_signals = []
    features = []
    labels = []
    n_epochs = len(df) // WINDOW_SIZE
    num_channels = len(signal_channels)
    
    logging.info(f"Processing {n_epochs} epochs from DataFrame with {len(df)} samples")
    logging.info(f"Using {num_channels} channels: {signal_channels}")
    logging.info(f"Using label column: {label_column}")
    
    for i in range(n_epochs):
        start = i * WINDOW_SIZE
        end = start + WINDOW_SIZE
        epoch_df = df.iloc[start:end]
        
        # Epoch í¬ê¸° ê²€ì¦
        if len(epoch_df) < WINDOW_SIZE:
            logging.warning(f"Epoch {i}: insufficient samples ({len(epoch_df)} < {WINDOW_SIZE})")
            continue
            
        # ë¼ë²¨ majority voting
        labels_epoch = epoch_df[label_column].map(map_sleep_stage).dropna().astype(int).values
        if len(labels_epoch) == 0:
            logging.warning(f"Epoch {i}: no valid labels found")
            continue
            
        label_counts = Counter(labels_epoch)
        major_label, count = label_counts.most_common(1)[0]
        ratio = count / len(labels_epoch)
        
        if ratio < majority_ratio:
            logging.debug(f"Epoch {i}: majority ratio {ratio:.2f} < {majority_ratio}, skipping")
            continue
            
        # ì‹ í˜¸ ì¶”ì¶œ (ë™ì  ì±„ë„ ìˆ˜)
        try:
            signals = []
            for channel in signal_channels:
                if channel not in epoch_df.columns:
                    logging.warning(f"Epoch {i}: missing channel {channel}")
                    continue
                signals.append(epoch_df[channel].values)
            
            if len(signals) != num_channels:
                logging.warning(f"Epoch {i}: expected {num_channels} channels, got {len(signals)}")
                continue
            
            # NaN ê°’ ê²€ì‚¬ ë° ì²˜ë¦¬
            signals = [np.nan_to_num(sig, nan=0.0) for sig in signals]
            signals = np.stack(signals, axis=0)  # shape: (num_channels, 1920)
            
            # ì‹ í˜¸ í’ˆì§ˆ ê²€ì‚¬ (ëª¨ë“  ê°’ì´ 0ì´ ì•„ë‹Œì§€ í™•ì¸)
            if np.all(signals == 0):
                logging.warning(f"Epoch {i}: all signals are zero, skipping")
                continue
            
            # ìˆ˜ì •: 5ê°œ HR/HRV í”¼ì²˜ ëª¨ë‘ ê³„ì‚° (BVP ì±„ë„ì´ ìˆëŠ” ê²½ìš°)
            hr_feature = 0.0
            rmssd_feature = 0.0
            peak_count_feature = 0.0
            ibi_mean_feature = 0.0
            ibi_std_feature = 0.0
            
            if 'bvp' in signal_channels:
                bvp_index = signal_channels.index('bvp')
                bvp_signal = signals[bvp_index]
                
                # 9ê°œ HR/HRV í”¼ì²˜ ëª¨ë‘ ê³„ì‚° (ì‹œê°„ + ì£¼íŒŒìˆ˜ ì˜ì—­)
                all_features_dict = calculate_all_features(bvp_signal, sampling_rate=SAMPLING_RATE)
                hr_feature = all_features_dict['hr']
                rmssd_feature = all_features_dict['rmssd']
                peak_count_feature = all_features_dict['peak_count']
                ibi_mean_feature = all_features_dict['ibi_mean']
                ibi_std_feature = all_features_dict['ibi_std']
                vlf_feature = all_features_dict['vlf']
                lf_feature = all_features_dict['lf']
                hf_feature = all_features_dict['hf']
                lf_hf_ratio_feature = all_features_dict['lf_hf_ratio']
                
                logging.debug(f"Epoch {i}: HR={hr_feature:.1f}, RMSSD={rmssd_feature:.3f}, PeakCount={peak_count_feature}, IBI_mean={ibi_mean_feature:.3f}, IBI_std={ibi_std_feature:.3f}, VLF={vlf_feature:.3f}, LF={lf_feature:.3f}, HF={hf_feature:.3f}, LF/HF={lf_hf_ratio_feature:.3f}")
            
            # ìˆ˜ì •: 9ê°œ í”¼ì²˜ ëª¨ë‘ í¬í•¨í•œ ë°°ì—´ ìƒì„± [HR, RMSSD, PeakCount, IBI_mean, IBI_std, VLF, LF, HF, LF/HF_ratio]
            feature_vector = np.array([hr_feature, rmssd_feature, peak_count_feature, ibi_mean_feature, ibi_std_feature, vlf_feature, lf_feature, hf_feature, lf_hf_ratio_feature], dtype=np.float32)
                
            raw_signals.append(signals)
            features.append(feature_vector)
            labels.append(major_label)
            
        except Exception as e:
            logging.warning(f"Error extracting signals for epoch {i}: {e}")
            continue
    
    logging.info(f"Successfully extracted {len(raw_signals)} valid epochs with HR/HRV features")
    return np.array(raw_signals), np.array(features), np.array(labels)


def load_dreamt_data(data_dir, majority_ratio=MAJORITY_RATIO, signal_channels=None, label_column=None):
    """
    DREAMT ë°ì´í„° ë¡œë“œ (raw signal + features + labels ëª¨ë‘ ë°˜í™˜)
    
    Args:
        data_dir (str): DREAMT ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        majority_ratio (float): majority voting ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.6)
        signal_channels (list): ì‚¬ìš©í•  ì‹ í˜¸ ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: CURRENT_SIGNAL_CHANNELS)
        label_column (str): ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: CURRENT_LABEL_COLUMN)
        
    Returns:
        tuple: (raw_signals, features, labels) 
            - raw_signals: np.array of shape (N, num_channels, 1920) - ì›ì‹œ ì‹ í˜¸ ë°ì´í„°
            - features: np.array of shape (N, 9) - HR/HRV í”¼ì²˜ (9ê°œ ê°’)
            - labels: np.array of shape (N,) - ìˆ˜ë©´ ë‹¨ê³„ ë¼ë²¨
    """
    if signal_channels is None:
        signal_channels = CURRENT_SIGNAL_CHANNELS
    if label_column is None:
        label_column = CURRENT_LABEL_COLUMN
        
    # ìˆ˜ì •: features ë¦¬ìŠ¤íŠ¸ ì¶”ê°€í•˜ì—¬ 3ê°œ ë°°ì—´ ëª¨ë‘ ìˆ˜ì§‘
    X_list, X_features_list, y_list = [], [], []
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    
    if not csv_files:
        logging.warning(f"No CSV files found in {data_dir}")
        # ìˆ˜ì •: 3ê°œ ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ í˜•íƒœ í†µì¼
        return np.array([]), np.array([]), np.array([])
        
    logging.info(f"Found {len(csv_files)} CSV files in {data_dir}")
    
    for csv_file in csv_files:
        logging.info(f"Reading file: {os.path.basename(csv_file)}")
        try:
            df = pd.read_csv(csv_file)
            
            # ì»¬ëŸ¼ëª… ì†Œë¬¸ì í†µì¼ ë° ê³µë°± ì œê±°
            df.columns = [c.lower().strip() for c in df.columns]
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦ (í˜„ì¬ ì‚¬ìš©í•  ì±„ë„ + ë¼ë²¨ ì»¬ëŸ¼)
            required_cols = signal_channels + [label_column]
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                logging.warning(f"Missing columns in {os.path.basename(csv_file)}: {missing_cols}")
                continue
                
            # ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            for col in signal_channels:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                
            # NaN ê°’ì´ ë„ˆë¬´ ë§ì€ ê²½ìš° ì œì™¸
            nan_counts = df[signal_channels].isna().sum()
            if nan_counts.max() > len(df) * 0.1:  # 10% ì´ìƒ NaNì´ë©´ ì œì™¸
                logging.warning(f"Too many NaN values in {os.path.basename(csv_file)}, skipping")
                continue
                
            raw_signals, features, labels = extract_epochs_with_features_from_df(df, majority_ratio=majority_ratio, 
                                        signal_channels=signal_channels, label_column=label_column)
            
            if len(raw_signals) > 0:
                X_list.append(raw_signals)
                X_features_list.append(features)  # ìˆ˜ì •: featuresë„ ìˆ˜ì§‘
                y_list.append(labels)
                logging.info(f"Successfully loaded {len(raw_signals)} epochs from {os.path.basename(csv_file)}")
            else:
                logging.warning(f"No valid epochs extracted from {os.path.basename(csv_file)}")
                
        except Exception as e:
            logging.warning(f"Error loading {os.path.basename(csv_file)}: {e}")
            continue
    
    if X_list:
        raw_signals = np.concatenate(X_list, axis=0)
        # ìˆ˜ì •: features ë°°ì—´ë„ concatenateí•˜ì—¬ ìƒì„±
        features_all = np.concatenate(X_features_list, axis=0)
        labels = np.concatenate(y_list, axis=0)
        logging.info(f"Total loaded: {len(raw_signals)} epochs from {len(X_list)} files")
        logging.info(f"Features shape: {features_all.shape}")  # ìˆ˜ì •: features ì •ë³´ ë¡œê¹…
        # ìˆ˜ì •: 3ê°œ ë°°ì—´ ëª¨ë‘ ë°˜í™˜
        return raw_signals, features_all, labels
    else:
        logging.error("No valid data loaded from any files")
        # ìˆ˜ì •: 3ê°œ ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ í˜•íƒœ í†µì¼
        return np.array([]), np.array([]), np.array([])


def load_actual_data(data_dir, majority_ratio=MAJORITY_RATIO, signal_channels=None, label_column=None):
    # ì‹¤ì œ ë°ì´í„°ë„ DREAMT í¬ë§·ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    return load_dreamt_data(data_dir, majority_ratio, signal_channels, label_column)


class SleepRawDataset(Dataset):
    """Raw signal ê¸°ë°˜ ìˆ˜ë©´ ë°ì´í„°ì…‹ (X: [num_channels,1920], y: int)"""
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.int64)
        
        # ë°ì´í„° ê²€ì¦
        if len(self.X) != len(self.y):
            raise ValueError(f"X and y lengths don't match: {len(self.X)} vs {len(self.y)}")
        if len(self.X.shape) != 3 or self.X.shape[2] != 1920:
            raise ValueError(f"Expected X shape (N, num_channels, 1920), got {self.X.shape}")
            
        self.num_channels = self.X.shape[1]
        logging.info(f"Dataset initialized with {len(self.X)} samples, shape: {self.X.shape}")
        
    def __len__(self):
        return len(self.X)
        
    def __getitem__(self, idx):
        # (num_channels,1920) -> (1920,num_channels) for LSTM (seq_len, feature_dim)
        x = torch.from_numpy(self.X[idx].T)  # shape: (1920, num_channels)
        y = torch.tensor(self.y[idx])
        return x, y


class SleepDualInputDataset(Dataset):
    """
    ë“€ì–¼ ì…ë ¥ ìˆ˜ë©´ ë°ì´í„°ì…‹: ì›ì‹œ ì‹ í˜¸ì™€ HR/HRV í”¼ì²˜ë¥¼ ë™ì‹œì— ì œê³µ
    
    ì´ ë°ì´í„°ì…‹ì€ ë‘ ê°€ì§€ ì…ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
    1. raw_signals: ì›ì‹œ ì‹ í˜¸ ë°ì´í„° (BVP, ê°€ì†ë„ ë“±)
    2. features: HR/HRV í”¼ì²˜ ë²¡í„° (ì‹¬ë°•ìˆ˜, RMSSD)
    """
    def __init__(self, raw_signals, features, labels):
        """
        Args:
            raw_signals: ì›ì‹œ ì‹ í˜¸ ë°ì´í„° (N, num_channels, 1920)
            features: HR/HRV í”¼ì²˜ (N, 9) - [HR, RMSSD, PeakCount, IBI_mean, IBI_std, VLF, LF, HF, LF/HF_ratio]
            labels: ìˆ˜ë©´ ë‹¨ê³„ ë¼ë²¨ (N,)
        """
        self.raw_signals = raw_signals.astype(np.float32)
        self.features = features.astype(np.float32)
        self.labels = labels.astype(np.int64)
        
        # ë°ì´í„° ê²€ì¦
        if len(self.raw_signals) != len(self.features) or len(self.raw_signals) != len(self.labels):
            raise ValueError(f"Data lengths don't match: raw_signals={len(self.raw_signals)}, "
                           f"features={len(self.features)}, labels={len(self.labels)}")
        
        if len(self.raw_signals.shape) != 3 or self.raw_signals.shape[2] != 1920:
            raise ValueError(f"Expected raw_signals shape (N, num_channels, 1920), got {self.raw_signals.shape}")
        
        # ìˆ˜ì •: í”¼ì²˜ ì°¨ì›ì„ 9ë¡œ ë³€ê²½ (HR, RMSSD, PeakCount, IBI_mean, IBI_std, VLF, LF, HF, LF/HF_ratio)
        if len(self.features.shape) != 2 or self.features.shape[1] != 9:
            raise ValueError(f"Expected features shape (N, 9), got {self.features.shape}")
        
        self.num_channels = self.raw_signals.shape[1]
        logging.info(f"SleepDualInputDataset initialized with {len(self.raw_signals)} samples")
        logging.info(f"  Raw signals shape: {self.raw_signals.shape}")
        logging.info(f"  Features shape: {self.features.shape}")
        logging.info(f"  Labels shape: {self.labels.shape}")
    
    def __len__(self):
        return len(self.raw_signals)
    
    def __getitem__(self, idx):
        """
        ë°ì´í„° ìƒ˜í”Œ ë°˜í™˜
        
        Returns:
            tuple: (x_raw, x_features, y)
                - x_raw: ì›ì‹œ ì‹ í˜¸ (1920, num_channels) - LSTMìš©
                - x_features: HR/HRV í”¼ì²˜ (9,) - MLPìš©
                - y: ë¼ë²¨ (ìŠ¤ì¹¼ë¼)
        """
        # ì›ì‹œ ì‹ í˜¸: (num_channels, 1920) -> (1920, num_channels) for LSTM
        x_raw = torch.from_numpy(self.raw_signals[idx].T)  # shape: (1920, num_channels)
        
        # HR/HRV í”¼ì²˜: (9,) -> (9,) for MLP
        x_features = torch.from_numpy(self.features[idx])   # shape: (9,)
        
        # ë¼ë²¨
        y = torch.tensor(self.labels[idx])
        
        return x_raw, x_features, y


class RawLSTMClassifier(nn.Module):
    """LSTM ê¸°ë°˜ ìˆ˜ë©´ ë‹¨ê³„ ë¶„ë¥˜ê¸° (raw signal ì…ë ¥)"""
    def __init__(self, input_size=None, hidden_size=64, num_layers=2, num_classes=NUM_CLASSES, dropout=0.2):
        super().__init__()
        
        # input_sizeê°€ Noneì´ë©´ CURRENT_SIGNAL_CHANNELSì˜ ê¸¸ì´ ì‚¬ìš©
        if input_size is None:
            input_size = len(CURRENT_SIGNAL_CHANNELS)
            
        self.input_size = input_size
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers=num_layers, 
            batch_first=True, 
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size*2, num_classes)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
        
        logging.info(f"LSTM model initialized with input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}")
        
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
                
    def forward(self, x):
        # x: (batch, seq_len, input_size)
        out, _ = self.lstm(x)
        out = out[:, -1, :]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
        out = self.dropout(out)
        out = self.fc(out)
        return out


class DualInputLSTMClassifier(nn.Module):
    """
    ë“€ì–¼ ì…ë ¥ LSTM ë¶„ë¥˜ê¸°: ì›ì‹œ ì‹ í˜¸ì™€ HR/HRV í”¼ì²˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸
    
    ì´ ëª¨ë¸ì€ ë‘ ê°€ì§€ ì…ë ¥ì„ ë°›ì•„ ìˆ˜ë©´ ë‹¨ê³„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤:
    1. x_raw: ì›ì‹œ ì‹ í˜¸ ë°ì´í„° (BVP, ê°€ì†ë„ ë“±)
    2. x_features: HR/HRV í”¼ì²˜ ë²¡í„° (ì‹¬ë°•ìˆ˜, RMSSD, í”¼í¬ìˆ˜, IBI í‰ê· , IBI í‘œì¤€í¸ì°¨)
    """
    def __init__(self, raw_input_size=None, feature_input_size=9, lstm_hidden_size=64, 
                 lstm_num_layers=2, mlp_hidden_size=32, num_classes=NUM_CLASSES, dropout=0.2):
        super().__init__()
        
        # ì…ë ¥ í¬ê¸° ì„¤ì •
        if raw_input_size is None:
            raw_input_size = len(CURRENT_SIGNAL_CHANNELS)
        self.raw_input_size = raw_input_size
        self.feature_input_size = feature_input_size
        
        # === LSTM ë¸Œëœì¹˜: ì›ì‹œ ì‹ í˜¸ ì²˜ë¦¬ ===
        # ì–‘ë°©í–¥ LSTMìœ¼ë¡œ ì›ì‹œ ì‹ í˜¸ì˜ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
        self.lstm = nn.LSTM(
            input_size=raw_input_size,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True,
            bidirectional=True,  # ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ ì •ë³´ í™œìš©
            dropout=dropout if lstm_num_layers > 1 else 0
        )
        
        # LSTM ì¶œë ¥ ì°¨ì›: ì–‘ë°©í–¥ì´ë¯€ë¡œ hidden_size * 2
        lstm_output_size = lstm_hidden_size * 2
        
        # === MLP ë¸Œëœì¹˜: HR/HRV í”¼ì²˜ ì²˜ë¦¬ ===
        # ì²« ë²ˆì§¸ ë ˆì´ì–´: í”¼ì²˜ ë²¡í„°ë¥¼ ì¤‘ê°„ ì°¨ì›ìœ¼ë¡œ í™•ì¥
        self.mlp_layer1 = nn.Linear(feature_input_size, mlp_hidden_size)
        self.mlp_activation = nn.ReLU()
        self.mlp_dropout = nn.Dropout(dropout)
        
        # ë‘ ë²ˆì§¸ ë ˆì´ì–´: ì¤‘ê°„ ì°¨ì›ì„ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì§• ì¶”ì¶œ
        self.mlp_layer2 = nn.Linear(mlp_hidden_size, mlp_hidden_size)
        
        # === ê²°í•© ë° ë¶„ë¥˜ ===
        # LSTMê³¼ MLP ì¶œë ¥ì„ ê²°í•©
        combined_size = lstm_output_size + mlp_hidden_size
        
        # ìµœì¢… ë¶„ë¥˜ ë ˆì´ì–´
        self.classifier = nn.Linear(combined_size, num_classes)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
        
        logging.info(f"DualInputLSTMClassifier initialized:")
        logging.info(f"  Raw input size: {raw_input_size}")
        logging.info(f"  Feature input size: {feature_input_size}")
        logging.info(f"  LSTM hidden size: {lstm_hidden_size}")
        logging.info(f"  MLP hidden size: {mlp_hidden_size}")
        logging.info(f"  Combined size: {combined_size}")
        logging.info(f"  Num classes: {num_classes}")
    
    def _init_weights(self):
        """ëª¨ë“  ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì ì ˆí•˜ê²Œ ì´ˆê¸°í™”"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'lstm' in name:
                    # LSTM ê°€ì¤‘ì¹˜ëŠ” ê¸°ë³¸ ì´ˆê¸°í™” ì‚¬ìš© (PyTorch ë‚´ì¥)
                    continue
                else:
                    # Linear ë ˆì´ì–´ëŠ” Xavier ì´ˆê¸°í™”
                    nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                # í¸í–¥ì€ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                nn.init.constant_(param, 0)
    
    def forward(self, x_raw, x_features):
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        
        Args:
            x_raw: ì›ì‹œ ì‹ í˜¸ ë°ì´í„° (batch_size, seq_len, raw_input_size)
            x_features: HR/HRV í”¼ì²˜ ë²¡í„° (batch_size, feature_input_size)
            
        Returns:
            logits: í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì ìˆ˜ (batch_size, num_classes)
        """
        batch_size = x_raw.size(0)
        
        # === LSTM ë¸Œëœì¹˜ ì²˜ë¦¬ ===
        # x_raw: (batch, seq_len, raw_input_size) -> LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(x_raw)
        # lstm_out: (batch, seq_len, lstm_hidden_size * 2)
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš© (ì‹œê³„ì—´ ì •ë³´ì˜ ìµœì¢… ìš”ì•½)
        lstm_final = lstm_out[:, -1, :]  # (batch, lstm_hidden_size * 2)
        
        # === MLP ë¸Œëœì¹˜ ì²˜ë¦¬ ===
        # ì²« ë²ˆì§¸ ë ˆì´ì–´: í”¼ì²˜ ë²¡í„° í™•ì¥
        mlp_out = self.mlp_layer1(x_features)  # (batch, mlp_hidden_size)
        mlp_out = self.mlp_activation(mlp_out)  # ReLU í™œì„±í™”
        mlp_out = self.mlp_dropout(mlp_out)    # Dropout ì ìš©
        
        # ë‘ ë²ˆì§¸ ë ˆì´ì–´: íŠ¹ì§• ì¶”ì¶œ
        mlp_out = self.mlp_layer2(mlp_out)     # (batch, mlp_hidden_size)
        mlp_out = self.mlp_activation(mlp_out)  # ReLU í™œì„±í™”
        mlp_out = self.mlp_dropout(mlp_out)    # Dropout ì ìš©
        
        # === ë‘ ë¸Œëœì¹˜ ì¶œë ¥ ê²°í•© ===
        # torch.catìœ¼ë¡œ LSTMê³¼ MLP ì¶œë ¥ì„ ì—°ê²°
        combined_features = torch.cat([lstm_final, mlp_out], dim=1)
        # combined_features: (batch, lstm_hidden_size * 2 + mlp_hidden_size)
        
        # === ìµœì¢… ë¶„ë¥˜ ===
        logits = self.classifier(combined_features)
        # logits: (batch, num_classes)
        
        return logits


def pretrain_on_dreamt(data_dir, output_path, epochs=EPOCHS_PRETRAIN, batch_size=BATCH_SIZE, lr=LR_PRETRAIN, majority_ratio=MAJORITY_RATIO, signal_channels=None, label_column=None, class_weights=None):
    """
    DREAMT ë°ì´í„°ë¡œ Pre-train
    
    Args:
        data_dir (str): DREAMT ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_path (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        lr (float): í•™ìŠµë¥ 
        majority_ratio (float): majority voting ë¹„ìœ¨
        signal_channels (list): ì‚¬ìš©í•  ì‹ í˜¸ ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: CURRENT_SIGNAL_CHANNELS)
        label_column (str): ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: CURRENT_LABEL_COLUMN)
    """
    if signal_channels is None:
        signal_channels = CURRENT_SIGNAL_CHANNELS
    if label_column is None:
        label_column = CURRENT_LABEL_COLUMN
        
    logging.info("Loading DREAMT data...")
    X, y = load_dreamt_data(data_dir, majority_ratio, signal_channels, label_column)
    
    if len(X) == 0:
        logging.error("No valid data found!")
        return
        
    logging.info(f"Loaded {len(X)} samples")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    unique, counts = np.unique(y, return_counts=True)
    logging.info("Class distribution:")
    for label, count in zip(unique, counts):
        logging.info(f"  {INV_LABEL_MAP[label]}: {count} ({count/len(y)*100:.1f}%)")
    
    # ë°ì´í„° ë¶„í•  (stratify ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    train_dataset = SleepRawDataset(X_train, y_train)
    test_dataset = SleepRawDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë™ì  input_size)
    input_size = len(signal_channels)
    model = RawLSTMClassifier(input_size=input_size, hidden_size=64, num_layers=2, num_classes=NUM_CLASSES)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    logging.info(f"Using device: {device}")
    
    # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì„¤ì • (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)
    if class_weights is None:
        # ìë™ ê³„ì‚°: ì ì€ ìƒ˜í”Œì— ë†’ì€ ê°€ì¤‘ì¹˜
        class_counts = np.bincount(y_train)
        total_samples = len(y_train)
        class_weights = total_samples / (len(class_counts) * class_counts)
    else:
        # ì‚¬ìš©ì ì§€ì • ê°€ì¤‘ì¹˜ ì‚¬ìš©
        class_weights = np.array(class_weights)
    
    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    logging.info(f"Class weights: {class_weights}")
    
    criterion = nn.CrossEntropyLoss(weight=weights)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Early stopping ë³€ìˆ˜
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    logging.info("Starting pre-training...")
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        train_loss /= len(train_loader)
        val_loss /= len(test_loader)
        accuracy = 100 * correct / total
        
        # Early stopping ì²´í¬
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            
        if patience_counter >= 10:  # Early stopping patience
            logging.info(f'Early stopping at epoch {epoch+1}')
            break
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ epochë§ˆë‹¤)
        logging.info(f'Epoch [{epoch+1}/{epochs}]')
        logging.info(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
        logging.info(f'Accuracy: {accuracy:.2f}%')
    
    # ìµœì ì˜ ëª¨ë¸ ìƒíƒœ ë³µì› ë° ì €ì¥
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        torch.save(best_model_state, output_path)
        logging.info(f"Pre-trained model saved to {output_path}")
    
    # === í‰ê°€ ë©”íŠ¸ë¦­ ì¶œë ¥ ===
    logging.info("\nConfusion Matrix (Validation Set):")
    print(confusion_matrix(all_targets, all_preds))
    logging.info("\nClassification Report (Validation Set):")
    print(classification_report(all_targets, all_preds, digits=4, target_names=CLASS_NAMES_4))


def pretrain_on_dreamt_dual_input(data_dir, output_path, epochs=EPOCHS_PRETRAIN, batch_size=BATCH_SIZE, 
                                 lr=LR_PRETRAIN, majority_ratio=MAJORITY_RATIO, signal_channels=None, 
                                 label_column=None, class_weights=None):
    """
    DREAMT ë°ì´í„°ë¡œ ë“€ì–¼ ì…ë ¥ ëª¨ë¸ Pre-train (ì›ì‹œ ì‹ í˜¸ + HR/HRV í”¼ì²˜)
    
    Args:
        data_dir (str): DREAMT ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_path (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        lr (float): í•™ìŠµë¥ 
        majority_ratio (float): majority voting ë¹„ìœ¨
        signal_channels (list): ì‚¬ìš©í•  ì‹ í˜¸ ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: CURRENT_SIGNAL_CHANNELS)
        label_column (str): ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: CURRENT_LABEL_COLUMN)
        class_weights (array): í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: None, ìë™ ê³„ì‚°)
    """
    if signal_channels is None:
        signal_channels = CURRENT_SIGNAL_CHANNELS
    if label_column is None:
        label_column = CURRENT_LABEL_COLUMN
        
    logging.info("=== ë“€ì–¼ ì…ë ¥ ëª¨ë¸ë¡œ DREAMT ë°ì´í„° ë¡œë”© ì‹œì‘ ===")
    
    # ìˆ˜ì •: ì¤‘ë³µëœ ë°ì´í„° ë¡œë”© ë¡œì§ ì œê±°í•˜ê³  load_dreamt_data í•¨ìˆ˜ ì‚¬ìš©
    raw_signals, features, labels = load_dreamt_data(
        data_dir, majority_ratio, signal_channels, label_column
    )
    
    if len(raw_signals) == 0:
        logging.error("No valid data loaded from any files!")
        return
    
    logging.info(f"=== ì „ì²´ ë°ì´í„° ë¡œë”© ì™„ë£Œ ===")
    logging.info(f"Total epochs: {len(raw_signals)}")
    logging.info(f"Raw signals shape: {raw_signals.shape}")
    logging.info(f"Features shape: {features.shape}")
    logging.info(f"Labels shape: {labels.shape}")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    unique, counts = np.unique(labels, return_counts=True)
    logging.info("Class distribution:")
    for label, count in zip(unique, counts):
        logging.info(f"  {INV_LABEL_MAP[label]}: {count} ({count/len(labels)*100:.1f}%)")
    
    # ë°ì´í„° ë¶„í•  (stratify ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€)
    raw_signals_train, raw_signals_test, features_train, features_test, labels_train, labels_test = train_test_split(
        raw_signals, features, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # ë“€ì–¼ ì…ë ¥ ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = SleepDualInputDataset(raw_signals_train, features_train, labels_train)
    test_dataset = SleepDualInputDataset(raw_signals_test, features_test, labels_test)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    # ë“€ì–¼ ì…ë ¥ ëª¨ë¸ ì´ˆê¸°í™”
    raw_input_size = len(signal_channels)
    # ìˆ˜ì •: feature_input_sizeë¥¼ 9ë¡œ ë³€ê²½ (HR, RMSSD, PeakCount, IBI_mean, IBI_std, VLF, LF, HF, LF/HF_ratio)
    feature_input_size = 9
    model = DualInputLSTMClassifier(
        raw_input_size=raw_input_size,
        feature_input_size=feature_input_size,
        lstm_hidden_size=64,
        lstm_num_layers=2,
        mlp_hidden_size=32,
        num_classes=NUM_CLASSES,
        dropout=0.2
    )
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    logging.info(f"Using device: {device}")
    
    # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì„¤ì • (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)
    if class_weights is None:
        # [ìˆ˜ì •] í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ì‹ì„ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë³€ê²½í•˜ì—¬ ì •ë°€ë„-ì¬í˜„ìœ¨ ê· í˜• ì¡°ì ˆ
        class_counts = np.bincount(labels_train)
        class_weights = 1.0 / np.log1p(class_counts)
        # ë¬´í•œëŒ€ ê°’ ë°©ì§€
        class_weights[np.isinf(class_weights)] = 1.0
    else:
        # ì‚¬ìš©ì ì§€ì • ê°€ì¤‘ì¹˜ ì‚¬ìš©
        class_weights = np.array(class_weights)
    
    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    logging.info(f"Class weights: {class_weights}")
    
    criterion = nn.CrossEntropyLoss(weight=weights)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Early stopping ë³€ìˆ˜
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    logging.info("=== ë“€ì–¼ ì…ë ¥ ëª¨ë¸ Pre-training ì‹œì‘ ===")
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0
        for batch_data in train_loader:
            # ë“€ì–¼ ì…ë ¥ ë°ì´í„° ì–¸íŒ¨í‚¹
            batch_x_raw, batch_x_features, batch_y = batch_data
            batch_x_raw, batch_x_features, batch_y = batch_x_raw.to(device), batch_x_features.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            # ë“€ì–¼ ì…ë ¥ ëª¨ë¸ì— ë‘ ì…ë ¥ ì „ë‹¬
            outputs = model(batch_x_raw, batch_x_features)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch_data in test_loader:
                # ë“€ì–¼ ì…ë ¥ ë°ì´í„° ì–¸íŒ¨í‚¹
                batch_x_raw, batch_x_features, batch_y = batch_data
                batch_x_raw, batch_x_features, batch_y = batch_x_raw.to(device), batch_x_features.to(device), batch_y.to(device)
                
                # ë“€ì–¼ ì…ë ¥ ëª¨ë¸ì— ë‘ ì…ë ¥ ì „ë‹¬
                outputs = model(batch_x_raw, batch_x_features)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        train_loss /= len(train_loader)
        val_loss /= len(test_loader)
        accuracy = 100 * correct / total
        
        # Early stopping ì²´í¬
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            
        if patience_counter >= 10:  # Early stopping patience
            logging.info(f'Early stopping at epoch {epoch+1}')
            break
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ epochë§ˆë‹¤)
        logging.info(f'Epoch [{epoch+1}/{epochs}]')
        logging.info(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
        logging.info(f'Accuracy: {accuracy:.2f}%')
    
    # ìµœì ì˜ ëª¨ë¸ ìƒíƒœ ë³µì› ë° ì €ì¥
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        torch.save(best_model_state, output_path)
        logging.info(f"Dual input pre-trained model saved to {output_path}")
    
    # === í‰ê°€ ë©”íŠ¸ë¦­ ì¶œë ¥ ===
    logging.info("\n=== ë“€ì–¼ ì…ë ¥ ëª¨ë¸ ìµœì¢… ì„±ëŠ¥ ===")
    logging.info("Confusion Matrix (Validation Set):")
    print(confusion_matrix(all_targets, all_preds))
    logging.info("Classification Report (Validation Set):")
    # ìˆ˜ì •: ë²„ê·¸ ìˆ˜ì • - all_targetsë¥¼ all_predsë¡œ ë³€ê²½í•˜ì—¬ ì •í™•í•œ í‰ê°€ ê°€ëŠ¥
    print(classification_report(all_targets, all_preds, digits=4, target_names=CLASS_NAMES_4))
    
    return model


def finetune_on_actual(actual_data_dir, pretrained_path, output_path, epochs=EPOCHS_FINETUNE, batch_size=BATCH_SIZE, lr=LR_FINETUNE, majority_ratio=MAJORITY_RATIO, signal_channels=None, label_column=None):
    """
    ì‹¤ì œ ë°ì´í„°ë¡œ Fine-tune
    
    Args:
        actual_data_dir (str): ì‹¤ì œ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        pretrained_path (str): Pre-trained ëª¨ë¸ ê²½ë¡œ
        output_path (str): Fine-tuned ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        lr (float): í•™ìŠµë¥ 
        majority_ratio (float): majority voting ë¹„ìœ¨
        signal_channels (list): ì‚¬ìš©í•  ì‹ í˜¸ ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: CURRENT_SIGNAL_CHANNELS)
        label_column (str): ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: CURRENT_LABEL_COLUMN)
    """
    if signal_channels is None:
        signal_channels = CURRENT_SIGNAL_CHANNELS
    if label_column is None:
        label_column = CURRENT_LABEL_COLUMN
        
    logging.info("Loading actual data...")
    X, y = load_actual_data(actual_data_dir, majority_ratio, signal_channels, label_column)
    
    if len(X) == 0:
        logging.error("No valid data found!")
        return
        
    logging.info(f"Loaded {len(X)} samples")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    unique, counts = np.unique(y, return_counts=True)
    logging.info("Class distribution:")
    for label, count in zip(unique, counts):
        logging.info(f"  {INV_LABEL_MAP[label]}: {count} ({count/len(y)*100:.1f}%)")
    
    # ë°ì´í„° ë¶„í•  (stratify ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ê· í˜• ìœ ì§€)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    train_dataset = SleepRawDataset(X_train, y_train)
    test_dataset = SleepRawDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    # ëª¨ë¸ ì´ˆê¸°í™” ë° pre-trained ê°€ì¤‘ì¹˜ ë¡œë“œ (ë™ì  input_size)
    input_size = len(signal_channels)
    model = RawLSTMClassifier(input_size=input_size, hidden_size=64, num_layers=2, num_classes=NUM_CLASSES)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    logging.info(f"Using device: {device}")
    
    if os.path.exists(pretrained_path):
        model.load_state_dict(torch.load(pretrained_path, map_location=device))
        logging.info(f"Loaded pre-trained weights from {pretrained_path}")
    else:
        logging.warning("Pre-trained model not found, starting from scratch")
    
    # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)
    class_counts = np.bincount(y_train)
    total_samples = len(y_train)
    class_weights = total_samples / (len(class_counts) * class_counts)
    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    
    criterion = nn.CrossEntropyLoss(weight=weights)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Early stopping ë³€ìˆ˜
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    logging.info("Starting fine-tuning...")
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        train_loss /= len(train_loader)
        val_loss /= len(test_loader)
        accuracy = 100 * correct / total
        
        # Early stopping ì²´í¬
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            
        if patience_counter >= 10:  # Early stopping patience
            logging.info(f'Early stopping at epoch {epoch+1}')
            break
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ epochë§ˆë‹¤)
        logging.info(f'Epoch [{epoch+1}/{epochs}]')
        logging.info(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
        logging.info(f'Accuracy: {accuracy:.2f}%')
    
    # ìµœì ì˜ ëª¨ë¸ ìƒíƒœ ë³µì› ë° ì €ì¥
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        torch.save(best_model_state, output_path)
        logging.info(f"Fine-tuned model saved to {output_path}")


if __name__ == "__main__":
    print("=== ìˆ˜ë©´ ë‹¨ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
    
    try:
        # ìµœì¢… ë“€ì–¼ ì…ë ¥ ëª¨ë¸ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        pretrain_on_dreamt_dual_input(
            data_dir=r"C:\\dreamt_pretrain",
            output_path="dreamt_pretrained_4class_9features.pth"
        )
        print("\nğŸ‰ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def test_ppg_analysis():
    """
    PPG ì‹ í˜¸ ë¶„ì„ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    print("=== PPG ì‹ í˜¸ ë¶„ì„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ìš© PPG ì‹ í˜¸ ìƒì„± (ê°€ìƒì˜ ì‹¬ë°•ìˆ˜ íŒ¨í„´)
    sampling_rate = 64.0
    duration = 30  # 30ì´ˆ
    t = np.linspace(0, duration, int(sampling_rate * duration))
    
    # ê¸°ë³¸ ì‹¬ë°•ìˆ˜: 60 BPM (1ì´ˆë§ˆë‹¤ í”¼í¬)
    base_hr = 60
    base_period = 60.0 / base_hr  # 1ì´ˆ
    
    # ê°€ìƒ PPG ì‹ í˜¸ ìƒì„± (ì—¬ëŸ¬ ì£¼íŒŒìˆ˜ ì„±ë¶„ í¬í•¨)
    ppg_signal = (
        0.5 * np.sin(2 * np.pi * base_hr / 60 * t) +  # ê¸°ë³¸ ì‹¬ë°•ìˆ˜
        0.2 * np.sin(2 * np.pi * 2 * base_hr / 60 * t) +  # 2ì°¨ ê³ ì¡°íŒŒ
        0.1 * np.sin(2 * np.pi * 3 * base_hr / 60 * t) +  # 3ì°¨ ê³ ì¡°íŒŒ
        0.05 * np.random.randn(len(t))  # ë…¸ì´ì¦ˆ
    )
    
    print(f"ìƒì„±ëœ PPG ì‹ í˜¸ ê¸¸ì´: {len(ppg_signal)} ìƒ˜í”Œ")
    print(f"ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sampling_rate} Hz")
    print(f"ì‹ í˜¸ ì§€ì†ì‹œê°„: {duration}ì´ˆ")
    
    # í”¼í¬ ê²€ì¶œ í…ŒìŠ¤íŠ¸
    print("\n--- í”¼í¬ ê²€ì¶œ í…ŒìŠ¤íŠ¸ ---")
    peak_indices, peak_properties = find_ppg_peaks(ppg_signal, sampling_rate=sampling_rate)
    print(f"ê²€ì¶œëœ í”¼í¬ ìˆ˜: {len(peak_indices)}")
    if len(peak_indices) > 0:
        print(f"ì²« ë²ˆì§¸ í”¼í¬ ìœ„ì¹˜: {peak_indices[0]} ìƒ˜í”Œ")
        print(f"ë§ˆì§€ë§‰ í”¼í¬ ìœ„ì¹˜: {peak_indices[-1]} ìƒ˜í”Œ")
    
    # HRê³¼ HRV í”¼ì²˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
    print("\n--- HR/HRV í”¼ì²˜ ê³„ì‚° í…ŒìŠ¤íŠ¸ ---")
    hr_hrv_features = calculate_all_features(ppg_signal, sampling_rate=sampling_rate)
    
    print("ê³„ì‚°ëœ í”¼ì²˜:")
    for key, value in hr_hrv_features.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    # ì´ë¡ ê°’ê³¼ ë¹„êµ
    expected_hr = base_hr
    actual_hr = hr_hrv_features['hr']
    hr_error = abs(actual_hr - expected_hr)
    
    print(f"\n--- ì •í™•ë„ ê²€ì¦ ---")
    print(f"ì˜ˆìƒ ì‹¬ë°•ìˆ˜: {expected_hr} BPM")
    print(f"ì‹¤ì œ ì‹¬ë°•ìˆ˜: {actual_hr:.1f} BPM")
    print(f"ì˜¤ì°¨: {hr_error:.1f} BPM")
    
    if hr_error < 5:  # 5 BPM ì´ë‚´ ì˜¤ì°¨ë©´ ì„±ê³µ
        print("âœ… ì‹¬ë°•ìˆ˜ ê²€ì¶œ ì„±ê³µ!")
    else:
        print("âŒ ì‹¬ë°•ìˆ˜ ê²€ì¶œ ì‹¤íŒ¨ - íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”")
    
    return ppg_signal, hr_hrv_features


def test_epoch_extraction_with_features():
    """
    ìƒˆë¡œìš´ epoch ì¶”ì¶œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    print("\n=== Epoch ì¶”ì¶œ + í”¼ì²˜ ê³„ì‚° í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ìš© DataFrame ìƒì„±
    sampling_rate = 64.0
    duration = 60  # 60ì´ˆ (2ê°œ epoch)
    n_samples = int(sampling_rate * duration)
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    t = np.linspace(0, duration, n_samples)
    
    # BVP ì‹ í˜¸ (ì‹¬ë°•ìˆ˜ ë³€í™” í¬í•¨)
    bvp_signal = (
        0.5 * np.sin(2 * np.pi * 60 / 60 * t) +  # 60 BPM
        0.2 * np.sin(2 * np.pi * 120 / 60 * t) +  # 2ì°¨ ê³ ì¡°íŒŒ
        0.1 * np.random.randn(n_samples)  # ë…¸ì´ì¦ˆ
    )
    
    # ê°€ì†ë„ ì‹ í˜¸ë“¤
    acc_x = 0.1 * np.sin(2 * np.pi * 0.5 * t) + 0.05 * np.random.randn(n_samples)
    acc_y = 0.1 * np.cos(2 * np.pi * 0.5 * t) + 0.05 * np.random.randn(n_samples)
    acc_z = 0.1 * np.sin(2 * np.pi * 1.0 * t) + 0.05 * np.random.randn(n_samples)
    
    # ìˆ˜ë©´ ë‹¨ê³„ ë¼ë²¨ (30ì´ˆë§ˆë‹¤ ë³€í™”)
    sleep_stages = []
    for i in range(n_samples):
        if i < n_samples // 2:
            sleep_stages.append('W')  # ì²« 30ì´ˆ: ê¹¨ì–´ìˆìŒ
        else:
            sleep_stages.append('N2')  # í›„ 30ì´ˆ: N2 ìˆ˜ë©´
    
    # DataFrame ìƒì„±
    test_df = pd.DataFrame({
        'bvp': bvp_signal,
        'acc_x': acc_x,
        'acc_y': acc_y,
        'acc_z': acc_z,
        'sleep_stage': sleep_stages
    })
    
    print(f"í…ŒìŠ¤íŠ¸ DataFrame ìƒì„± ì™„ë£Œ: {len(test_df)} ìƒ˜í”Œ")
    print(f"ì‚¬ìš©í•  ì±„ë„: {CURRENT_SIGNAL_CHANNELS}")
    print(f"Epoch í¬ê¸°: {WINDOW_SIZE} ìƒ˜í”Œ ({WINDOW_DURATION}ì´ˆ)")
    
    # ìƒˆë¡œìš´ epoch ì¶”ì¶œ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    try:
        raw_signals, features, labels = extract_epochs_with_features_from_df(
            test_df, 
            majority_ratio=0.6,
            signal_channels=CURRENT_SIGNAL_CHANNELS,
            label_column=CURRENT_LABEL_COLUMN
        )
        
        print(f"\nâœ… Epoch ì¶”ì¶œ ì„±ê³µ!")
        print(f"ì¶”ì¶œëœ epoch ìˆ˜: {len(raw_signals)}")
        print(f"Raw signals shape: {raw_signals.shape}")
        print(f"Features shape: {features.shape}")
        print(f"Labels shape: {labels.shape}")
        
                    # ì²« ë²ˆì§¸ epochì˜ í”¼ì²˜ í™•ì¸ (9ê°œ í”¼ì²˜ ëª¨ë‘ í‘œì‹œ)
        if len(features) > 0:
            first_hr = features[0][0]
            first_rmssd = features[0][1]
            first_peak_count = features[0][2]
            first_ibi_mean = features[0][3]
            first_ibi_std = features[0][4]
            first_label = INV_LABEL_MAP.get(labels[0], 'Unknown')
            
            print(f"\nì²« ë²ˆì§¸ epoch ì •ë³´:")
            print(f"  ë¼ë²¨: {first_label}")
            print(f"  ì‹¬ë°•ìˆ˜: {first_hr:.1f} BPM")
            print(f"  RMSSD: {first_rmssd:.3f}")
            print(f"  í”¼í¬ ìˆ˜: {first_peak_count}")
            print(f"  IBI í‰ê· : {first_ibi_mean:.3f}ì´ˆ")
            print(f"  IBI í‘œì¤€í¸ì°¨: {first_ibi_std:.3f}ì´ˆ")
        
        return raw_signals, features, labels
        
    except Exception as e:
        print(f"âŒ Epoch ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None, None, None


def test_dual_input_model():
    """
    ë“€ì–¼ ì…ë ¥ ëª¨ë¸ì˜ ë™ì‘ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    print("\n=== ë“€ì–¼ ì…ë ¥ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
    batch_size = 4
    seq_len = 1920
    num_channels = 4
    # ìˆ˜ì •: feature_sizeë¥¼ 9ë¡œ ë³€ê²½ (9ê°œ HR/HRV í”¼ì²˜)
    feature_size = 9
    num_classes = 4
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    x_raw = torch.randn(batch_size, seq_len, num_channels)      # ì›ì‹œ ì‹ í˜¸
    x_features = torch.randn(batch_size, feature_size)          # HR/HRV í”¼ì²˜
    y = torch.randint(0, num_classes, (batch_size,))           # ë¼ë²¨
    
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
    print(f"  x_raw shape: {x_raw.shape}")
    print(f"  x_features shape: {x_features.shape}")
    print(f"  y shape: {y.shape}")
    
    # ë“€ì–¼ ì…ë ¥ ëª¨ë¸ ìƒì„±
    try:
        model = DualInputLSTMClassifier(
            raw_input_size=num_channels,
            feature_input_size=feature_size,
            lstm_hidden_size=32,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ê²Œ ì„¤ì •
            lstm_num_layers=1,    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ
            mlp_hidden_size=16,   # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ê²Œ ì„¤ì •
            num_classes=num_classes,
            dropout=0.1
        )
        
        print(f"\nâœ… ë“€ì–¼ ì…ë ¥ ëª¨ë¸ ìƒì„± ì„±ê³µ!")
        print(f"ëª¨ë¸ êµ¬ì¡°:")
        print(f"  LSTM: {num_channels} -> {32*2} (ì–‘ë°©í–¥)")
        print(f"  MLP: {feature_size} -> {16} -> {16}")
        print(f"  ê²°í•©: {32*2 + 16} -> {num_classes}")
        
        # ëª¨ë¸ì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ë‹¬
        model.eval()
        with torch.no_grad():
            outputs = model(x_raw, x_features)
            
        print(f"\nâœ… ìˆœì „íŒŒ ì„±ê³µ!")
        print(f"  ì…ë ¥: x_raw {x_raw.shape}, x_features {x_features.shape}")
        print(f"  ì¶œë ¥: {outputs.shape}")
        print(f"  ì˜ˆì¸¡ í´ë˜ìŠ¤: {torch.argmax(outputs, dim=1)}")
        
        # ì†ì‹¤ ê³„ì‚° í…ŒìŠ¤íŠ¸
        criterion = nn.CrossEntropyLoss()
        loss = criterion(outputs, y)
        print(f"  í…ŒìŠ¤íŠ¸ ì†ì‹¤: {loss.item():.4f}")
        
        return model, x_raw, x_features, y
        
    except Exception as e:
        print(f"âŒ ë“€ì–¼ ì…ë ¥ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None, None, None, None
